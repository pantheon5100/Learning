<h1>Train Trick</h1>

[TOC]

# 1.学习率和batchsize如何影响模型的性能 ？

## 1.1 WHY ?

随机梯度算法原理如下：
$$
w_{t+1}=w_{t}-\eta \frac{1}{n} \sum_{x \in \mathcal{B}} \nabla l\left(x, w_{t}\right)\tag{1}
$$
n为BatchSize，$\eta$ 为LearningRate。从上式，这两个因子直接决定了模型的权重更新，从优化本身来看他们是影响模型性能收敛最重要的参数。

LR直接影响模型的收敛状态，BS则影响模型的泛化能力==（理由？）==，两者又是分子分母的直接关系。

## 1.2 HOW LR ?

要达到一个强的凸函数的最小值，学习率的调整应该满足：
$$
\begin{array}{l}{\sum_{i=1}^{\infty} \epsilon_{i}=\infty}
\\ {\sum_{i=1}^{\infty} \epsilon_{i}^{2}<\infty}\end{array}\tag{2}
$$
第一个式子决定了不管初始状态里最优状态有多远，总是可以收敛。第二个式子约束了学习率随着训练进行有效地降低，保证收敛的稳定性，各种自适应学习率算法本子上就是不断调整各个时刻的学习率。

学习率决定了权重的迭代步长，因此是一个非常敏感的参数，主要有两个方面，==第一个是初始学习率的大小，第二个是学习率的变换方案。==

### 1.2.1 初始学习率大小对模型性能的影响

初始学习率肯定有一个最优值，过大则导致模型不收敛，过小则导致模型收敛特别慢或者无法学习，下图展示了不同大小的学习率下模型收敛情况的可能性。

![1557502228025](C:\Users\saber\Documents\agit\Learning\1557502228025.png)

通常采用最简单的搜索法，即从小到大开始训练模型，然后记录损失的变化

![1557502298264](C:\Users\saber\Documents\agit\Learning\1557502298264.png)

通常选择0.1,0.01等。

随着学习率的增加，模型也可能会从欠拟合过度到过拟合状态，在大型数据集上的表现尤其明显，笔者之前在Place365上使用DPN92层的模型进行过实验。随着学习率的增强，模型的训练精度增加，直到超过验证集。

### 1.2.2 学习率变换策略对模型性能的影响



学习率在模型的训练过程中很少有不变的，通常会有两种方式对学习率进行更改，一种是**预设规则学习率变化法**，一种是**自适应学习率变换方法**。

#### 1.2.2.1 预设规则学习率变化法

常见策略包括fixed，step，exp，inv，multistep，poly，sigmoid等

![1557502593543](C:\Users\saber\Documents\agit\Learning\1557502593543.png)

![1557502614227](C:\Users\saber\Documents\agit\Learning\1557502614227.png)

- **step，multistep方法的收敛效果最好**，这也是我们平常用它们最多的原因。虽然学习率的变化是最离散的，但是并不影响模型收敛到比较好的结果。



- 其次是**exp，poly**。它们能取得与step，multistep相当的结果，也是因为学习率以比较好的速率下降，**操作的确很骚，不过并不见得能干过step和multistep。**



- inv和fixed的收敛结果最差。这是比较好解释的，因为fixed方法始终使用了较大的学习率，而inv方法的学习率下降过程太快，这一点，当我们直接使用0.001固定大小的学习率时可以得到验证，最终收敛结果与inv相当。

从上面的结果可以看出，对于采用非自适应学习率变换的方法，学习率的绝对值对模型的性能有较大影响，研究者常使用step变化策略。



目前学术界也在探索一些最新的研究方法，比如cyclical learning rate，示意图如下：

![1557584968738](C:\Users\saber\Documents\agit\Learning\1557584968738.png)

**实验证明通过设置上下界，让学习率在其中进行变化，可以在模型迭代的后期更有利于克服因为学习率不够而无法跳出鞍点的情况。**

确定学习率可以使用 LR range test方法



# 2. BatchNorm 与 Dropout 共存

参考：[批归一化和Dropout不能共存？这篇研究说可以](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650762305&idx=4&sn=de66853d537f7ebaf4901528f312303b&chksm=871aa83fb06d21296f708827ecf0a8309602a5f4b98e3f1c82e0046e9404700d8c062ad340a2&xtrack=1&scene=90&subscene=93&sessionid=1558141516&clicktime=1558141554&ascene=56&devicetype=android-28&version=2700043a&nettype=WIFI&abtest_cookie=BAABAAoACwASABMABgAjlx4Av5keANyZHgD0mR4AAZoeAAOaHgAAAA%3D%3D&lang=zh_CN&pass_ticket=2z8w9XeUBlSl9SMjbuoFYI%2FsxfiJ%2FF9WbZZ0t42WaP0qT%2Bn5%2BUdvNBlD%2B4qJ5iD7&wx_header=1)

## 2.1 动机

寻找一种百花每一个输入层的高校计算方法。

白化（whitening）神经网络的输入能够实现较快的收敛速度，但众所周知，独立的激活函数必须白化。

研究者试图使每一个权重层的网络激活函数更加独立。最近的神经科学发现表明：神经系统的表征能力随群体中独立神经元数量的增加而线性增加（Moreno-Bote et al., 2014; Beaulieu-Laroche et al., 2018），这一发现可以支持以上做法。因此，研究者使得权重层的输入更加独立。独立的激活函数的确使得训练过程更加稳定。

生成独立组件的直观解决方案是引入一个附加层，该层在激活函数上执行独立组件分析（ICA）

为了解决这一棘手的问题，研究者发现批归一化和 Dropout 可以结合在一起，为每个中间权重层中的神经元构建独立的激活函数。

**该研究的主要贡献如下：**

- 在其新提出的独立组件（Independent Component，IC）层中结合了两种流行的技术：批归一化和 Dropout。该 IC 层可以降低任意一对神经元之间的交互信息和相关系数，这能加快网络的收敛速度。
- 为了证实该理论分析，研究者在 CIFAR10/100 和 ILSVRC2012 数据集上进行了广泛的实验。实验结果证明，本文的实现在三个方面提升了当下网络的分类性能：i) 更稳定的训练过程，ii) 更快的收敛速度，iii) 更好的收敛极限。

## 2.2 如何结合BN、DP？


为表述方便，本文将 {-BatchNorm-Dropout-} 表示为独立组件（IC）层。IC 层以一种连续的方式将每对神经元分开，应用 IC 层可以使得神经元更加独立。本文所用的方法可以直观地解释为：

> BatchNorm 归一化网络激活函数，==使它们的均值和单位方差为零==，就像 ZCA 方法一样。Dropout 通过在一个层中为神经元引入独立的随机门来构造独立的激活函数，允许神经元以概率 p 输出其值，否则输出 0 来停用它们。直观上来说，一个神经元的输出传递的信息很少一部分来自其他神经元。因此，我们可以假设这些神经元在统计上是彼此独立的。3.1 节在理论上证明，本文中提到的 IC 层可以将任意两个神经元输出之间的相互信息减少 p^2 倍，相关系数减少 p，其中 p 为 Dropout 概率。作者表示，据他们所知，以前从未有研究者提出 Dropout 的这种用法。

与 ICA 和 ZCA 等传统的无监督学习方法不同，研究者不需要从独立特征中恢复原始信号或保证这些特征的独特性，只需要提取一些有用特征即可，这样有助于实现监督学习任务。他们的分析表明，提出的 ==IC 层应置于权重层而非激活层之前==。

BatchNorm 的此类用法依然阻止网络参数在梯度方向上进行更新，而这是实现最小损失的最快方法，并且呈现出一种曲折的优化行为。与之前批归一化和 Dropout 在激活层之前同时被使用不同，本文研究者提出，批归一化和 Dropout 的作用类似于 ICA 方法，所以应置于权重层之前。如此一来，训练深度神经网络时会实现更快的收敛速度。理论分析和实验结果表明，批归一化和 Dropout 应结合作为 IC 层，这样将来能够广泛应用于训练深度网络。

![1558142445396](C:\Users\saber\Documents\agit\Learning\Train_Trick.assets\1558142445396.png)

*图 1：（a）在权重层和激活层之间执行白化运算（或称为 BatchNorm）的常见做法。（b）研究者提出将 IC 层置于权重层之前。*

IC 层可以用几行 Python 代码轻松实现，如下图所示：

![1558142614061](C:\Users\saber\Documents\agit\Learning\Train_Trick.assets\1558142614061.png)

*论文链接：https://arxiv.org/pdf/1905.05928.pdf*